{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">01. Intro to Pandas</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. What is Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examples of groupby, pivot, tidying, timeseries-resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">02. Selecting Subsets of Data</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pandas Intro\n",
    "* What is Pandas\n",
    "* Why numpy is faster\n",
    "* %timeit\n",
    "* Index | Columns | Data(values)\n",
    "* Columns or 1 | rows or 0\n",
    "* type(x) -->  pandas.core.frame.DataFrame ---> Package | Subpackage | Modules | Class\n",
    "* Common Data Types (Boolean, float, integer, Object, DateTime)\n",
    "* Missing values (NaN | None(object) | NaT)\n",
    "* Converting object columns into DateTime by using `parse_dates`\n",
    "* Different bit of memory used for different data types \n",
    "* **Metadata** - size, shape, info(), info(memory_usage='deep'), dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting a Meaningful Index\n",
    "* Extract Index: df.index | Extract Columns: df.columns | Extract Data: df.values\n",
    "* Same extractions can be done for series\n",
    "* Use `index_col` to set the index and read df simultaneously `movie = pd.read_csv('../data/movie.csv', index_col='title')`\n",
    "* Index can be selected by providing a range of slices eg-`idx2[100:120:4]`, or a list of index values eg- `idx2[list]`\n",
    "* We can set index later (after reading the dataframe) by using `set_index`\n",
    "* Increasing column width etc use `pd.options.display.` To reset we have to use `pd.reset_option('all')`\n",
    "* To change number of columns `pd.set_option('display.max_columns', 40, 'display.max_rows', 8)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Making the most of a Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Selecting Subsets of Data from DataFrames with just the brackets\n",
    "* Select subsets of data by using `[ ]` | `.loc` | `iloc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Selecting Subsets of Data from DataFrames with `.loc`\n",
    "3 way selection\n",
    "* In case of only 1 row and 1 column selection: `df.loc[row_name, column_name]`\n",
    "* In case of multiple row and multiple column selection.\n",
    "    * Step 1. Make a list of rows to be selected with variable name `rows` and list of columns with variable name `columns`\n",
    "    * Step 2. `df.loc[rows, columns]`\n",
    "* In case of a sequence of row selection use slicing. **For slicing we do NOT need extra square brackette** i.e. `df.loc[row1:row5, column_name]`. Here we are providing a list of columns\n",
    "\n",
    "\n",
    "* We can also provide step-size while using `.loc`\n",
    "\n",
    "\n",
    "* **Weird part of `.loc`**: `.loc` always includes the last value when slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Selecting Subsets of Data from DataFrames with `.iloc`\n",
    "Same 3 way selection as `.loc`. Instead of row/column names we will use integer location here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Selecting Subsets of Data - Series\n",
    "* Same `.loc` | `.iloc` selection of series similar to dataframe \n",
    "* Only `[ ]` selection is also possible like dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Boolean Indexing Single Conditions\n",
    "* For filtering `.iloc` does NOT work\n",
    "* `[ ]` or `.loc` has to be used in case of filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Boolean Indexing Multiple Conditions\n",
    "* Using `and (&)`, `or (|)`, and `not (~)` logical operators\n",
    "* `isin` use when filtering several conditions within a particular column. example ` movies['content_rating'].isin(['R', 'PG-13', 'PG'])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Boolean Indexing More\n",
    "* Convert a column containing DateTime into DateTime datatype by using `parse_dates`\n",
    "* To find whether column values exist between 2 numbers, use **`between`** i.e. `filt = series.between(n1, n2)` leads to boolean series. Then `df.loc[filt]` or `series.loc[filt]`\n",
    "* Use `.isin` if we are trying to filter by multiple elements of a column \n",
    "* `.isna` and `isnull` are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">02. Essential Commands</span>**\n",
    "\n",
    "**<span  style=\"color:red; font-size:30px\">Series</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Series Attributes and Statistical Methods\n",
    "* `series.size`\n",
    "* `series.index`\n",
    "* `series.values`\n",
    "\n",
    "\n",
    "**Aggregation methods:** While calling aggregation methods, pandas ignore missing values\n",
    "* `sum` example. `series.sum()` \n",
    "* `min`\n",
    "* `max`\n",
    "* `mean`\n",
    "* `median`\n",
    "* `std` - standard deviation\n",
    "* `var` - variance\n",
    "* `count` - returns number of non-na values | `len(series_name)` give total number (including null values\n",
    "* `describe` - returns most of the above aggregations in one Series\n",
    "* `quantile` - returns given percentile of distribution\n",
    "* Multiple aggregation in 1 time: `series.agg(['min', 'max', 'median'])`\n",
    "\n",
    "\n",
    "**NON-Aggregation methods:** This will return a new series\n",
    "* `abs` - takes absolute value\n",
    "* `round` - round to the nearest given decimal\n",
    "* `cummin` - cumulative minimum\n",
    "* `cummax` - cumulative maximum\n",
    "* `cumsum` - cumulative sum\n",
    "* `rank` - rank values in a variety of different ways\n",
    "* `diff` - difference between one element and another within a series\n",
    "* `pct_change` - percent change from one element to another in a series \n",
    "* `series.diff(-1)` or `series.pct_change(-1)` to (reverse it)\n",
    "\n",
    "==================================================================================================\n",
    "\n",
    "* Finding **percentile** of a series: `series.quantile(q=.999)` will output the 99.9th percentile of the series\n",
    "* In series, `skipna` is True by default. But we can change it to False for specific reasons.\n",
    "* For a series, if we need to find out biggest to lowest (vice-versa) and give a rank to it, use `rank`\n",
    "* `diff`- difference between one element and another in a series\n",
    "* Reverse the `diff` of a series \n",
    "* Substract a series element wise with stepsize by using `period` with `diff` or `pct_change`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Series Methods More\n",
    "**How to handle missing values**\n",
    "* **`isna`** - Returns a Series of booleans based on whether each value is missing or not\n",
    "* **`notna`** - Exact opposite of **`isna`**\n",
    "* **`fillna`** - fills missing values in a variety of ways\n",
    "* **`dropna`** - Drops the missing values from the Series\n",
    "* **`.count()`** method says number of **non-missing** values\n",
    "* **`.isna().mean()`** tells us % of missing values\n",
    "\n",
    "==================================================================================================\n",
    "* `.sort_values()` to sort series values\n",
    "* `.sort_index()` will sort series index\n",
    "* `.sample()` will select a part of the data by mentioning number. And `frac` of data can be also collected\n",
    "* `series.sample(10, replace=True)`: replace=True allows to repeat a value. Good way to make the sample size bigger than the original size\n",
    "* `.idxmax()` index of maximum series value\n",
    "* `.idxmin()` index of minimum series value\n",
    "* `.unique()` gives unique values of a series\n",
    "* `.nunique()` gives no. of unique values, By default, nunique does not count missing values i.e. NaN. \n",
    "* `drop_duplicates(keep='first')` removes duplicate inputs in the series, `drop_duplicates(keep=False)` will remove any duplicate value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. String Series Methods\n",
    "* `value_counts()` for object or string columns | However `value_counts()` works for all data types \n",
    "* `value_counts(normalize=True)` to get % instead of counts\n",
    "* Method specific for string column can be accessed by `series.str.method()`\n",
    " \n",
    " **`.str` specific methods** [str accessor API][1] \n",
    "* `series.str.count('x')` will count occurances of `x` or any other passed string\n",
    "* `series.str.contains('x')` will give a boolean series of whether `x` or any other passed string is present or not\n",
    "* `series.str.find('x')` will give lowest index value where `x` or any other passed string is present. -1 if that string is not found \n",
    "* `series.str.len()` gives length of string elements in the series\n",
    "\n",
    "\n",
    "\n",
    "* common pandas and python `.str` methods\n",
    "* `series.str.split()` split string on spaces or anything else that is passed to `split()`\n",
    "* **`series.str.split(expand=True)`** will cause splited string elements into a dataframe\n",
    "* replace `x` by `y` by using `series.str.replace('x', 'y')`\n",
    "\n",
    "[1]: http://pandas.pydata.org/pandas-docs/stable/reference/series.html#api-series-str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Datetime Series Methods\n",
    "* An entire column/series of datetime values is `datetime`. Each entity in that series is `timestamp`. Difference between 2 `datetime` series or`timestamp` generates `timedelta`\n",
    "\n",
    "* Use `parse_dates` to convert columns (that can be datetime) into a datetime column\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "* Use `series.dt.attributes` (example of attributes: `year`, `month`, `minute`, `dayofweek` etc) [Visit the API for datetime attributes][1]\n",
    "* Web-page embedding in jupyter notebook using `IFrame`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Datetime (`series.dt.methods`) methods: `ceil` | `floor` | `round` | `strftime` | `to_period` (use with [offset aliases][2]) `series.dt.ceil('H')`\n",
    "* Offset aliases example - `D` - day | `H` - hour  |  `T` or `min` - minute  | `S` - second\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Timedeltas** is an amount of time. Can be produced by substracting 2 datetime series. Can access `.dt`\n",
    "* **Period** represents a period of time eg- entire month or year or minute. `series.dt.to_period('M' OR any other offset alias)` to convert a datetime into period. Period also has `.dt` accessor\n",
    "* **[strftime][3]** stands for **str**ing **f**ormat **time**- converts a datetime to strings format time by using `series.dt.strftime('%A, %B %d, %Y at %X')`\n",
    "\n",
    "[1]: http://pandas.pydata.org/pandas-docs/stable/reference/series.html#api-series-dt\n",
    "\n",
    "[2]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
    "\n",
    "[3]: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:red; font-size:30px\">DataFrame</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. DataFrame Attributes and Methods\n",
    "* `index` | `columns` | `values` | `dtypes` | `shape`(rows, columns) | `size` (row x column)\n",
    "* deafult index is `rangeindex`\n",
    "* To selective specific datatype columns of a dataframe use `df.select_dtypes('number')` or `object` or `datetime` etc or provide a list `['int', 'object']`\n",
    "* `pd.options.display` chnage appearances\n",
    "* common methods and attributed of series and df\n",
    "* Unique methods of series and dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DataFrame Descriptive Statistic Methods\n",
    "\n",
    "**Aggregation methods:** While calling aggregation methods, pandas ignore missing values\n",
    "* Some of the following methods could be used to the entire df even with object columns. Just pass `numeric_only=True` as an argument inside the method\n",
    "* `sum`\n",
    "* `min`\n",
    "* `max`\n",
    "* `mean`\n",
    "* `median`\n",
    "* `std` - standard deviation\n",
    "* `var` - variance\n",
    "* `count` - returns number of non-na values\n",
    "* `describe` - returns most of the above aggregations in one Series\n",
    "* `quantile` - returns given percentile of distribution\n",
    "\n",
    "\n",
    "**NON-Aggregation methods:** This will return a new series\n",
    "* `abs` - takes absolute value\n",
    "* `round` - round to the nearest given decimal. Applicable to entire df. Will ignore object columns automatically\n",
    "* `cummin` - cumulative minimum\n",
    "* `cummax` - cumulative maximum\n",
    "* `cumsum` - cumulative sum\n",
    "* `rank` - rank values in a variety of different ways\n",
    "* `diff` - difference between one element and another\n",
    "* `pct_change` - percent change from one element to another in a series (reverse it)\n",
    "\n",
    "\n",
    "`axis`= VERTICALLY `axis = 0(or 'index')` | HORIZONTALLY `axis = 1(or 'columns')`\n",
    "`df.describe(include='object')` summary statistics on string columns <br>\n",
    "`df.describe()` summary statistics on numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. DataFrame Methods More\n",
    "* `sort_values()` | `sort_index()`\n",
    "* **Dealing with missing values:** `isna`, `notna`, `fillna`, `dropna`\n",
    "* Delete NaN rows `df.dropna()`, delete columns with NaN `df.dropna(axis='columns')`, delete rows based on NaN values of a particular column `df.dropna(subset=['content_rating'])`\n",
    "* `fillna` could be `ffill` / `bfill` with or without limit `df.fillna(method='ffill')` | `df.fillna(method='ffill', limit=1)`\n",
    "* Use dictionary to fillna multiple columns at once: `df.fillna({'col1': 'PG', 'col2': 199})`\n",
    "* Dropping all rows of a dataframe based on NaN values of `x` column: `df.dropna(subset=['x'])`\n",
    "* Finding corrosponding index of maximum and minimum values **`idxmax`** and **`idxmin`**\n",
    "* idxmax for all number columns `df.select_dtypes('number').idxmax()`\n",
    "* Rename column by **`rename`** `college.rename(columns={'col_name': 'new_col_name','col2': 'new_col_name2'})`\n",
    "* Drop column by **`df.drop(columns=[])`** and rows by **`drop(index=[])`**\n",
    "<br>\n",
    "<br>\n",
    "* Drop duplicate rows: **`drop_duplicates`**. `df.shape` and `df.drop_duplicates().shape` to check how many exact same rows are in the dataset\n",
    "* Dropping duplicate rows based on 1 columns: use `subset` with **`drop_duplicates`**\n",
    "* **`pd.to_numeric`** to coerce string columns to numeric data types\n",
    "* Sort dataframe by two columns `df.sort_values(by=['a', 'b'])`\n",
    "*  Sort dataframe by two columns in different direction`df.sort_values(by=['a', 'b'], ascending=[True, False])`\n",
    "* Finding index of maximum value of each numeric columns `df.select_dtypes('number').idxmax()`\n",
    "* `.is_alpha()` to check whether elements are string or not. But in case it is not string it will return NaN instead of False. So to convert it into boolean series, use `fillna` to False\n",
    "* Insert a series/column to a particular position in the dataframe by **`df.insert(position, new_column_name, values)`**\n",
    "* Series Methods not found in DataFrames - `str`, `dt`, `unique`, `value_counts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. DataFrame Methods More II\n",
    "* Drop duplicate rows: **`drop_duplicates`**. `df.shape` and `df.drop_duplicates().shape` to check how many exact same rows are in the dataset\n",
    "* use `subset` with **`drop_duplicates`** to delete rows when a values of a particular columns are repeated\n",
    "* **`CLIP`**: works on both df and series. It can put a lower and/or upper boundary to numeric columns example- `df.clip(50000, 100000)` OR `df.clip(upper = 50000)`\n",
    "* Insert a series/column to a particular position in the dataframe by **`df.insert(location, name of new column, series_Values)`** -- To find the location of the column after which we want to insert the new column use: `df.columns.get_loc('column name')`. **location** would be +1 to that\n",
    "* **`replace`** can work with both numeric and string columns in series or dataframe. `df.replace(to_replace=number/string, value=replacing string/number)`\n",
    "* For multiple **replace** in 1 step use a dictionary `df.replace(to_replace={'White':'WHITE', 50000: 11111, 70000: 99999})`\n",
    "* To replace **substring** (part of the text), we have to use **regex=True** `df.replace(to_replace={'Department':'dept'}, regex=True)`\n",
    "* `df.nlargest(5, 'salary')` , other than `sort_values` we can use `nlargest` or `nsmallest`\n",
    "* `df.sort_values('salary').drop_duplicates(subset=['race', 'sex'])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Changing Data types\n",
    "* pass `'float'` | `'int'` | `'str'` as an argument into Series/df.`astype()` eg- `df['col_name'].astype('str')` OR `df.astype('str')`\n",
    "* if we convert an int to bool, 0 will be False and rest numbers will True\n",
    "\n",
    "\n",
    "* If any numeric column has dtype as 'object' then `sort_values(ascending=False)` to check whether strings are really present in the column (strings will move up above numbers because numeric characters have lower unicode point tha alphabetic character.) Convert that kind of columns by using **`pd.to_numeric(objectseries/column_name, errors='coerce')`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: Calculating normality\n",
    "* Checking normality using Z-score\n",
    "* For a boolean series, `series.all()` returns True if all the series value is True otherwise False. `series.any()` will return True if any value in the series is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">03. Grouping</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Groupby Aggregation Basics\n",
    "* `df.groupby('<grouping column>').agg({'<aggregating column>':'<aggregating function>'})`\n",
    "* after grouping, use `.reset_index()` if required to \n",
    "* `.ngroups` produces number of groups\n",
    "\n",
    "\n",
    "* **Aggregation functions used with groupby**\n",
    "    + **`sum`**\n",
    "    + **`min`**\n",
    "    + **`max`**\n",
    "    + **`mean`**\n",
    "    + **`median`**\n",
    "    + **`std`**\n",
    "    + **`var`**\n",
    "    + **`count`** - count of non-missing values\n",
    "    + **`size`** - count of all elements\n",
    "    + **`first`** - first value in group\n",
    "    + **`last`** - last value in group\n",
    "    + **`idxmax`** - index of maximum value in group\n",
    "    + **`idxmin`** - index of minimum value in group\n",
    "    + **`any`** - checks for at least one True value - returns boolean\n",
    "    + **`all`** - checks for at least one False value - returns boolean\n",
    "    + **`unique`** Gives a list of unique values\n",
    "    + **`nunique`** - number of unique values in group\n",
    "    + **`sem`** - standard error of the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grouping and Aggregating with Multiple Columns\n",
    "* Grouping with multiple columns\n",
    "* To find `size` or number of total elements (Aggregation): `df.groupby(['race', 'gender']).size()`: This will give a series\n",
    "* `df.groupby(['race', 'sex']).size().reset_index(name='size')` will give a dataframe\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "* `df.groupby(['race', 'sex']).agg({col1:['min','max], col2:['idxmax','idxmin']})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Grouping with Pivot Tables [and Styling]\n",
    "* pivot_table is better alternative than performing groupby with 2 different columns\n",
    "* `df.pivot_table(index='col1', columns='col2', values='col3', aggfunc='mean')`\n",
    "    * `index` - grouping column\n",
    "    * `columns` - grouping column\n",
    "    * `values` - aggregating column\n",
    "    * `aggfunc` - aggregating function (defaulted to the mean)\n",
    "* For finding size, `values` is not needed `df.pivot_table(index='col1', columns='col2', aggfunc='size')`\n",
    "* Multiple columns `df.pivot_table(index=['col1','col2'],columns=['col3','col4'],values='salary',aggfunc='median')`\n",
    "    \n",
    "    \n",
    "* Use `astype('int')` | `round(x)` to reduce noise in resulting pivot table\n",
    "* `aggfunc` similar to Aggregation functions used with groupby\n",
    "\n",
    "\n",
    "* **Styling**: To highlight data\n",
    "    * `style.highlight_max()`\n",
    "    * `.style.highlight_max(axis=None)` gives the highest value of the entire dataframe | `axis=0`-highest value across each column | `axis=1` - highest value across each row\n",
    "    * To highlight both maximum and minumum value each row `.style.highlight_max(axis='columns').highlight_min(axis='columns', color='lightblue')`\n",
    "    * `style.background_gradient(cmap='YlOrRd')`\n",
    "    *  To put comma in numbers use `.style.format('{:,.0f}')`\n",
    "    \n",
    "    \n",
    "* Styling documentation: https://pandas.pydata.org/pandas-docs/stable/style.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Counting with Crosstabs\n",
    "* `pd.crosstab`\n",
    "* Frequency counting with a Series USE `value_counts()`\n",
    "\n",
    "\n",
    "* **Groupby**\n",
    "    * `df.groupby(['col1', 'col2']).size()`\n",
    "    \n",
    "* **Pivot Table**: Multiple column cab be used \n",
    "    * `df.pivot_table(index='col1', columns='col2', aggfunc='size', fill_value=0)`\n",
    "    * `df.pivot_table(index=['col1','col2'],columns=['col3','col4'],values='salary',aggfunc='median')`\n",
    "    * for size we do not have to put `values` parameter\n",
    "<br>\n",
    "<br>\n",
    "* **CrossTab**: only size and normalize can be done\n",
    "    * `pd.crosstab(index=df['col1'], columns=df['col2'])`\n",
    "    * Use `normalize='all'` | `'columns'` | `'index'`\n",
    "    * Use `margins=True`\n",
    "    \n",
    "    \n",
    "* Multi-index crosstabs\n",
    "    * list of index: `index=[df[col1], df[col2]]` | `cols = [df[col3], df[col4]]`\n",
    "    * Then crosstab: `pd.crosstab(index=index, columns=cols)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reshape MORE\n",
    "\n",
    "* If we have a multi-level index due to groupby, we can use `unstack` to make 1 of the index into columns \n",
    "* `df.groupby(['cola', 'colb']).size().unstack(level='colb', fill_value=0)`\n",
    "\n",
    "\n",
    "\n",
    "* Use **`chi2-contingency`** to check statistical significant between 2 groups after cross-tab\n",
    "    * **step1**. Do crosstab between 2 groups:`df=pd.crosstab(index=df['col1'], columns=df['col2'])`\n",
    "    * **Step2**. Check whether the difference between 2 groups are statistically significant by\n",
    "        * `from scipy.stats import chi2_contingency`\n",
    "        * `chi2_contingency(df)`: It wil give chi2 test statistic | p-value | degrees of freedom | expected counts\n",
    "        * if p-value is essentially 0, giving us tremendous confidence that these two group counts are indeed different. \n",
    "\n",
    "\n",
    "* Use `fill_value=0` everytime you see a `NaN` after groupby | pivoit_table | crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create Your Own Data Analysis\n",
    "#### Places to find data\n",
    "* [Kaggle datasets][1]\n",
    "* [data.world][2]\n",
    "* Most large US cities, [NYC][3], [Denver][4], [US Gov't][5], simply do a web search for \"open data [US city]\"\n",
    "\n",
    "\n",
    "\n",
    "[1]: https://www.kaggle.com/datasets\n",
    "[2]: https://data.world/\n",
    "[3]: https://opendata.cityofnewyork.us/\n",
    "[4]: https://www.denvergov.org/opendata\n",
    "[5]: https://www.data.gov/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Alternate GroupBy Syntax\n",
    "* Other syntaxes of groupby- DO NOT use / no need top remember\n",
    "* `size` | `count` for groupby -  we do NOT need to put a column under `.agg` example- `df.groupby(['col1', 'col2'}).size()` or use `count()` . For size it will generate only 1 column. For count it will generate multiple column representing not-null values of each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  8. Custom Aggregation\n",
    "* Writing customized aggregation function for series. Must output a single aggregated value. This can be also used for groupby\n",
    "    * `def min_max(s):`<br>\n",
    "      `return s.max() - s.min()`\n",
    "    * ex- `df.groupby('col1').agg({'col2': min_max})` custom function (only aggregation function possible) not within strings\n",
    "    * As we are passing `distance` that means we are passing a series of each group to the function, so custom function would be like<br> \n",
    "    `def min_max(sub_series):`<br>\n",
    "    `return sub_series.max()-sub_series.min()`\n",
    "    \n",
    "    * we can use lambda function like: `flights.groupby('airline').agg({'distance': lambda sub_series: sub_series.max()- sub_series.min()})`\n",
    "    \n",
    "    \n",
    "* Find first and last rows of each group `flights.groupby('airline').nth([0,-1])` - several other functions available in pandas https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Transform and Filter with GroupBy\n",
    "* **Filtering** -End result: Original dataframe with certein groups are filtered out\n",
    "    * Write custom functions that will return a boolean value. Here we pass sub_df example\n",
    "        * `def find_tot(sub_df):`\n",
    "        * `return sub_df[col2].sum()>15` then `df.groupby('Col').filter(find_tot)`\n",
    "        * `df.groupby('Col').filter(CUSTOM_FUCNTION)`\n",
    "        <BR>\n",
    "    \n",
    "        <BR>\n",
    "    * OR `df.groupby('Col').filter(lambda sub_df: sub_df['col2'].sum() > 15)` notice no `.agg` used\n",
    "    * Find actors that appeared in at least 25 movies--> Groupby actor, then groupby size of each group and select groups with size more than 25\n",
    "<BR>\n",
    "                        \n",
    "<BR>                      \n",
    "* **Transform** - do NOT aggregate rather TRANSFORM. Helpful when we want to divide each row of a group by the maximum value of that group. Or divide every value of a particular column in a group by the maximum value of the column in that group\n",
    "    * `df.groupby('item')['quantity'].transform(custom function)` As we want to transform a series ed-`quantity` here custom function should be sub_series like filter method---\n",
    "    * `def min_max(sub_series):`<br>\n",
    "      `return sub_series-sub_series.min()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra-Case Study: Counting Pandas [Series & Dataframe attributes/methods]\n",
    "* How to extract tables from a .html\n",
    "* How to extract tables from a html which contain a specific word using RegEx `match`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">05. Time-Series</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datetime(TimeStamp) and Timedelta\n",
    "* **date** - Just the Month, Day and Year.\n",
    "* **time** - Just the Hours, minutes, seconds and parts of a second (milli/micro/nano). \n",
    "\n",
    "\n",
    "* **DateTime | TimeStamp**:`DateTime`(Python) is `TimeStamp`(This is specific to Pandas)\n",
    "* Use `.dt` accessor with datetime series, NOT with individual datetime object\n",
    "* `pd.to_datetime` converts strings and numbers to Timestamps example: `pd.to_datetime('July 20, 1969 2:56 a.m. 15 seconds')`\n",
    "* datetime - Has both date (Year, Month, Day) and time (Hour, Minute, Second). Time goes upto Nanosecond\n",
    "* Make a datetime from epoch: `pd.to_datetime(20000, unit='d')` unit can be `'s'`, `'h'`, `'d'` or `pd.to_datetime('today')`. Then we can use attributes directly, no need to use `.dt` accessor like we do in case of a datetime series\n",
    "    * example `day = pd.to_datetime('Jan 15, 1997')`\n",
    "    * `day.day_name()`\n",
    "* **To covert a column into DateTime** to start with use: `pd.read_csv('x.csv', parse_dates=['hire_date'])`\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "* **TimeDelta**: Amount of time (dont include date part. Extends from date to nanoseconds). Substracting 2 datetime or timestamp will generate TimeDelta\n",
    "* `pd.to_timedelta('5days 10h')` converts strings and numbers to Timedeltas thus 1 year would be `pd.to_timedelta(1, unit='y')` unit can be `'s'`, `'h'`, `'d'` and **`'y'`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Intro to Time Series-DateTime index\n",
    "* JSON data from the IEX trading API `pd.read_json()`\n",
    "* Use `.dt` accessor\n",
    "* Convert datetime column to `datetime index` to get the advantages of an index\n",
    "    * Big advantage is **partial selection**, imagine index is in format `2017-04-06`. If we use `df.loc['2017-04-06']` then that row will be selected. If we use `df.loc['2017-04']` entire month would be selected. `df.loc['2017-01':'2017-02']` will select Jan and Feb of 2017. `df.loc['2017']` will select entire year. \n",
    "    \n",
    "    \n",
    "* **DateTime sampling** sampling possible ONLY if it is in index \n",
    "    * `weather = pd.read_csv('../data/weather.csv', parse_dates=['date'], index_col='date')`\n",
    "    * If we want to **sample** from the entire `datetime index` (example-last day of each month) we can use **`asfreq`**. This only works with **`DateTimeIndex`**\n",
    "    * We need to pass **offset alias** with `asfreq` http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases `df.asfreq('BA')` example for selecting final business day annually\n",
    "    * Based on need, we might have to pass **anchored offset alias** `df.asfreq('W-FRI')` example for sampling every friday. We can further customize by `df.asfreq('6W-FRI')` sampling every 6th Friday\n",
    "\n",
    "\n",
    "* **Upsampling and Downsampling**\n",
    "    * During upsampling (suppose data is for every day and I am sampling every 4 hour) it can lead to a lot of NaN value, use `ffill` to remove them example-`df.asfreq('4H', method='ffill')`\n",
    "\n",
    "\n",
    "* **Custom offset alias**: Custom business days (As some weekdays are holidays)\n",
    "\n",
    "\n",
    "* **Creating date ranges** example -`pd.date_range(start='1/1/2012', periods=10, freq='20S')` to  create 10 values begining with January 1, 2012 every 20 seconds. We can replace `periods` with `end` i.e. `pd.date_range(start='1/1/2012', end='10/1/2012', periods=8)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Grouping by Time\n",
    "* Grouping of time by **`resample`**. Resample should be passed with an **offset alias** [Link](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases)\n",
    "    * example- `df.resample('M').agg({'column': 'mean'})`   \n",
    "<p> </p>      \n",
    "* For grouping to work (resample), we need to have a Datetime Index or we have to pass a `on` parameter to `resample` specifying datetime column example-`df.resample('M', on='date_col').agg({'column': 'mean'})`   \n",
    "<p> </p>   \n",
    "* If we are grouping by a month or quarter then labelling(index) it as a single date would NOT make sense. We can label it as a **period** `df.resample('Q', kind='period').agg({'col': ['size', 'min']})`\n",
    "<p> </p>   \n",
    "* we can convert a datetime column to a period column `df['datetimecolumn'].dt.to_period('M')`. `.dt` accessor also works on period column\n",
    "<p> </p>   \n",
    "* **Anchored offset:** When we resample, it gets aggregated by a specific day (eg month end , or sunday for weekly aggregation). We can anchor by another day example- `df.resample('W-WED')` | or every 5 months (`df.resample('5M'`) | every 22 weeks anchored to Thursday (`.resample('22W-THU')`)\n",
    "<p> </p>\n",
    "* To resample based on a datetime column and then convert it into a period:<br>\n",
    "`df.resample('w', kind='period').agg({'column':'sum'})`\n",
    "<p> </p>\n",
    "* `.dt` accessor works for period columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Displaying any website in the jupyter notebook <br>\n",
    "`from IPython.display import IFrame`<br>\n",
    "`IFrame('url', width=800, height=500)`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Rolling Windows\n",
    "* Rolling windows is similar to resample but NOT exactly similar . \n",
    "\n",
    "* **Rolling windows rules**\n",
    "    * Use **datetime index** | or use `on` parameter\n",
    "    * for rolling windows dates should be sorted\n",
    "    * Try using days or `'D'` in offset alias. i.e. '`365D'` for 1year or 12 month rolling window\n",
    "    \n",
    "\n",
    "* `df.rolling('5D').agg({'column': ['mean', np.size]})`WITH offset alias\n",
    "\n",
    "\n",
    "* OR `df.rolling(5).agg({'column': ['mean', np.size]})` WITHOUT offset alias. In this case to prevent `NaN` use `min_periods=1`. We can use `center=True` i.e. `df.rolling(5, min_periods=3, center=True).agg({'column': ['mean', np.size]})`\n",
    "\n",
    "\n",
    "* **Series resample | rolling window** replace rolling by resample | we can use offset alias<br>\n",
    "    * `series.rolling(5).mean()` | `series.rolling(5).agg(['mean','min'])`\n",
    "    * Only rolling requires **`np.size`**. Resample can use `'size'`\n",
    "    \n",
    "\n",
    "* while reading a csv file, we can `parse_date` to make a column datetime column and use `index_col` the same column to make that an column an index in 1 line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Grouping by Time & another Column requires GROUPER\n",
    "* Alternative to `resample` is `groupby+Grouper`. \n",
    " * **1st step:** `tg = pd.Grouper(freq='5Y')`\n",
    " * **2nd step:** `df.groupby(tg).agg({'salary':'mean'})`\n",
    " * If datetime is just a column and not index then `tg = pd.Grouper(freq='5Y', on='date_col')`\n",
    " \n",
    " \n",
    "* For multi-level grouping. Use non-datetime column atfirst and then use datetime column.\n",
    "\n",
    "\n",
    "* **Group together** by using `grouper` and then `groupby` and **Grouping independently** by using `groupby` on non-datetime column and `resample` on date-time column might result **in different outputs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The 3 ways it can be done (Multi-level grouping)\n",
    "\n",
    " * **1st type:**`df.groupby('column1').resample('10A').agg({'column2':'mean'})`\n",
    " <p></p>\n",
    " * **2nd type:** `tg = pd.Grouper(freq='10Y')` <br>\n",
    "`groups = ['column1', tg]`<br>\n",
    "`df.groupby(groups).agg({'column2':'mean'})`\n",
    "<p></p>            \n",
    " * **3rd type:** For multi-level grouping we can just use `pivot_table`<br>\n",
    " `tg = pd.Grouper(freq='10Y')`<br>\n",
    " `df.pivot_table(index=tg, columns='column1', values='column2')` # `aggfunc=mean` by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Python  DateTime Module\n",
    "* Converting datetime into string by **strftime** | `dateutil` -- `relativedelta` an upgradation over `timedelta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">06. Regular Expression</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-04 Intro to Regular Expression-Character set & Grouping\n",
    "\n",
    "* **`contains`**: To check whether a match presents or not, use **`contains`**. This gives boolean series.<p></p> \n",
    "    * **Literal character matching**: `series.str.contains('star')` to match if series contains string 'star'  \n",
    "    * whether 'x', 'y', or 'z' present or not, use `series.str.contains('x') | series.str.contains('y') | series.str.contains('z')`  OR `series.str.contains('[xYZ]')`\n",
    "<p></p>  \n",
    "<p></p> \n",
    "    * **Special characters**: ` . ^ $ * + ? { } [ ] \\ | ( )`\n",
    "        * `.` dot matches any character. `series.str.contains('m.le')` will match male, mule, smile etc <p></p> \n",
    "        * `^` will force pattern to match at the beginning of the string. `series.str.contains('^m.le')` will match male, mule but NOT smile<p></p> \n",
    "        * `$` force pattern to match end of the string. `series.str.contains('war$')` will match civil war, not warcraft<p></p> \n",
    "        * combining special character `series.str.contains('^s.n')` will match san, sin, son, sanfrancisco etc <p></p> \n",
    "        * `*` matches **0 or more** eg  `series.str.contains('ah*no')` could be ano, ahno, ahhhhno. <p></p> \n",
    "        * `+` matches **1 or more** thus eg  `series.str.contains('ah+no')` could be ahno, ahhhhhno. <p></p> \n",
    "        * `?` matches **0 OR 1** time eg  `series.str.contains('ah?no')` could be ano, ahno<p></p> \n",
    "        * `a{3,}` matches 3 or more a in a row. `a{,3}` will match 0 to 3 a in a row. `a{3,5}` matches 3 to 5 a in a row<p></p> \n",
    "        * `|` matches OR condition `series.str.contains('Friend|Enemy')` will match any string which contains Friend OR Enemy in it<p></p> \n",
    "        * `[ ]` match one of the character inside it. `series.str.contains('T[aei]d')` would match Tad, Ted or Tid <p></p> \n",
    "        * `[0-9 ]` represent any digit | `[a-z ]` represent any lowercase character | `[A-Z]` represent any UPPERcase character | `[a-zA-Z]` represent any lower/upper case character.\n",
    "<p></p>  \n",
    "    * **Special characters within `[ ]` and `\\` loses it's meaning**: \n",
    "        * ` . ^ $ * + ? { } [ ] \\ | ( )` loses its meaning within `[ ]`. `[.]` means literal dot. `[*$]` means literal * and $ <p></p>\n",
    "        * ` . ^ $ * + ? { } [ ] \\ | ( )` loses its meaning within `\\`. **`\\*`** means literal *<p></p>\n",
    "        * `x[^aeiou]`: Here ^ within [ ] means it will match everything other than what is present within [ ]. x will be followed by anything other than aeiou. Similarly  `x[^A-Za-z]` means x will be followed by a digit<p></p> \n",
    "<p></p>  \n",
    "<p></p>  \n",
    "    * **`\\d`**: all digits <br>\n",
    "    **`\\D`**: all non-digits <br> \n",
    "    **`\\s`**: whitespace <br>\n",
    "    **`\\S`**: Non-whitespace <br>\n",
    "    **`\\w`**: word character (upper or lower), digit, underscore. equivalent to [A-Za-z0-9_] <br> **`\\W`**: any Non-word character eg- `^W+` startes with 1 or more non-word character<br>\n",
    "    **`\\b`**: word-boundary  eg - `'^(In|My)\\b'` will match words starting with In or My. `\\b` will ensure In or My are separate words and not part of a bigger word<br>\n",
    "    **`\\B`**: Non-word boundary<br>\n",
    "    * **`?:`** is **non-capturing group**. eg- `'^(?:In|My)\\b'`. Non-capturing group is useful for extraction\n",
    "<p></p>  \n",
    "<p></p>     \n",
    "*  **`count`**: `series.str.count('T[aei]d')` counts number of times the pattern appears\n",
    "<p></p>  \n",
    "<p></p> \n",
    "* **`extract`** or **`extractall`**: To extract the pattern use **`extract`** or **`extractall`**<br>\n",
    "    * **`extract`** Must have the pattern we want to extract within `( )` i.e. capturing group. If we want a pattern to match but not extract a part of that pattern to be extracted, use non-capturing group `(?:pattern)` <p></p>\n",
    "    *  `series.str.extract(pattern, expand=False)` will result into a series. If `expand=True`, it will generate a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study- Feature Engineering Titanic\n",
    "\n",
    "* to **extract first character** of a string series - \n",
    "    * `string_series=df['string_col']`<br>\n",
    "    * `string_series.str[0]` | **Finding length of string** `string_series.str.len()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">07. Tidy data </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tidy data with `melt`\n",
    "* Each variable (Like Male and Female should be within a single column and not form separate columns) should form a single column. Variable is anything that is liable to change\n",
    "\n",
    "\n",
    "* **Melting is done to stack 1 column below another**\n",
    "<p></p>\n",
    "* `df_melted = df.melt(id_vars='col_name', value_vars='col_list' var_name='new melted col name', value_name='new col_name' )`\n",
    "    * `id_vars`= to the column name that will not change from original df\n",
    "    * `value_vars` = should be the list of columns we want to stack 1 below another\n",
    "    * `var_name` = Name of the new melted column\n",
    "    * `value_name` = Name of the newly generated number column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reshape by `pivot` and `pivot_table`\n",
    "* **`pivot`**: Opposite of `melt` is `pivot`\n",
    "    * Pivot will put **unique elements of a single column into separate columns**\n",
    "<p></p>\n",
    "    * `df_pivot = df.pivot(index='col_name', columns='target_col', values='col')`\n",
    "    * `index`= column that will not change from original df and it will become new index\n",
    "    * `columns` = should be the target column whose unique elements needs to be separate columns\n",
    "    * `values` = Name of the new column whose values will be redistributed under new columns (after pivot)\n",
    "     <p></p>   \n",
    "    * After pivoting index and columns will have few extra labels. so follow the steps-\n",
    "        * `df_pivot.reset_index()`\n",
    "        * `df_pivot.rename_axis(None, axis='columns')`\n",
    "<p></p>\n",
    "\n",
    "* **`pivot_table`**: if there are duplicate rows, `pivot` won't work and we have to use `pivot_table`. Aggregation of the duplicate rows will be done by `pivot_table`\n",
    "<p></p>\n",
    "    * `df_pivot_table = df.pivot_table(index='col_name', columns='target_col', values='col', aggfunc='func')`\n",
    "    * `values`= the column that has numerical values that will be aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Common messy datasets\n",
    "* Various examples of messy datasets and how to clean them\n",
    "* If we have to keep multiple column constant, we can NOT use `pivot` we have to use `pivot_table`\n",
    "* `df_pivot_table = df.pivot_table(index='col_name', columns='target_col', values='col', aggfunc='max')` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Why Tidy data\n",
    "* Examples of when Tidy data is better\n",
    "\n",
    "\n",
    "# Case study: My brothers keeper\n",
    "* Messy data-> Divide dataframe into 2 part -> Melt and clean up -> Join the melted dataframe by\n",
    "`df1_melt.merge(df2_melt on=['col1,col2,col3]`. Here col1, col2, and col3 are the common columns in 2 dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">08. Joining Data </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Automatic Index Alignment\n",
    "* Pandas aligns the data by index (of series and dataframe both) atfirst and then carry forward the operations such as addition, subtraction\n",
    "\n",
    "* Uncommon index will lead to NaN after operations such as addition\n",
    "\n",
    "* Duplicate values in index will lead to cartesian product\n",
    "\n",
    "\n",
    "# 2. Combining data\n",
    "* **`pd.concat`** is used to concatenate df on top of each other OR side by side\n",
    "    * ** Concatenating on top of each other**:<br>\n",
    "    *  `pd.concat([df1, df2], ignore_index=True)`. To prevent original index use `ignore_index=True`\n",
    "<p></p>\n",
    "    * `pd.concat([df1, df2], keys=[df1_name, df2_name])`. To have an index stating which dataframe the data came from, we use `keys`\n",
    "<p></p>\n",
    "    * `pd.concat([df1, df2], join='inner'])` By default all columns will be kept after concatenating. So if there are some non-common columns between dataframes, then due to automatic alignement of columns will lead to NaN values. `join=inner` only keeps common columns\n",
    "<p></p>\n",
    "    * ** Concatenating side to side**:<br>\n",
    "    *  `pd.concat([df1, df2], axis=1)`\n",
    "    \n",
    "# 3. SQL databases\n",
    "\n",
    "\n",
    "# 4. Data Normalization\n",
    "* In a dataset sometimes some column values are very repititive. We can create a dimension with few columns that are repititive and somehow related to each other. \n",
    "\n",
    "* Next, remove dupliucate rows and add a primary key to that dimension\n",
    "\n",
    "* Now merge this dimension with original dataframe based on common columns.\n",
    "\n",
    "* Next drop these common columns from original dataframe, as based on the primary key of the dimension(now added to the original dataframe) I can explore the common columns in the dimension\n",
    "\n",
    "* This way we can prevent repititive data capture in the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">09. Visualization </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Matplotlib fundamentals\n",
    "\n",
    "* Basics of all controlling parts of a plot view\n",
    "* `fig, ax = plt.subplots(figsize=(a,b))`. The overall plotting part is `fig`. Graph part is `ax`\n",
    "    * Use `ax` to further optimize other properties-\n",
    "        * `ax.set_title('abc')` | `ax.set_xlabel('X axis')` | `ax.set_xlim(0,5)` \n",
    "        * `ax.set_xticks([1.5, 3, 4.5])` | `ax.set_xticklabels(['a', 'b', 'c'])`\n",
    "        * `ax.set_title('abc', size=, color=, background=,fontname=, rotation=)`\n",
    "        * Different colors https://matplotlib.org/gallery/color/named_colors.html\n",
    "        * `ax.tick_params()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Matplotlib Text and Lines\n",
    "* Adding text in a figure: `ax.text(x=, y=, s='text', color=, size=, rotation=, backgroundcolor=, fontname=)`\n",
    "* Set/change position by `text = ax.text(all top parameters)` --> `text.set_position((5,4))`\n",
    "* Horizontal line: `ax.hlines(y=, xmin=, xmax=)` |  Vertical line: `ax.vlines(x=, ymin=, ymax=)`\n",
    "* Common line properties: **linewidth, color, linestyle**\n",
    "* Grid line- `ax.grid(linestyle=dashed, color='brown', linewidth=3)` | add `axis='x'` if we want gridline only on x axis\n",
    "* **Annotate a point with arrow**:\n",
    "    * `ax.annotate('text', xy=(2,3), xytext=(4,7), arrowprops={'color':'blue'}, size=15)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Multiple axes figures\n",
    "* `fig, ax = plt.subplots(2,3,figsize=(a,b))` will generate 2 rows and 3 columns i.e. 6 plots\n",
    "* Now we can access all 6 plot specific axis as `ax[0,0]` to `ax[1,2]`\n",
    "* We can custom optimize each plot i.e. each `ax` based on 01. Matplotlib fundamentals \n",
    "\n",
    "# 04. Matplotlib data plotting\n",
    "* Several plots possible using`ax`\n",
    "    * `ax.plot(x, y, data=, marker='s', linestyle='--')` many other parameters can be used or altered.\n",
    "    * `ax.plot()` will generate a line plot. We can make `ax.scatter()` | `ax.bar()` | `ax.pie()`\n",
    "    \n",
    "* List of colors\n",
    "* List of markers\n",
    "* **Univariate analysis**: `ax.boxplot()`\n",
    "\n",
    "\n",
    "* **Plotting dates**: `ax.plot_date('datetime col', 'numeric_Col', data=)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Plotting with Pandas\n",
    "\n",
    "* Plot from dataframe or series directly with `series.plot()` | `dataframe.plot()`\n",
    "* `dataframe.plot()`: Pandas plotting is column based, plots 1 column 1 at a time. *X-axis is the index and y-axis is the column values. Column name will be legend*\n",
    "\n",
    "\n",
    "* **Change plot types**:\n",
    "    * `df.plot(kind='hist')` \n",
    "    * Other plot types are `line`(default), `bar`, `barh`, `hist`, `box`, `kde`, `area`, `pie`, `scatter`\n",
    "    \n",
    "* Additional plotting arguments that could be put inside `df.plot(kind=)` are\n",
    "    * `linestyle(ls)`, `linewidth(lw)`, `color(c)`, `alpha`, `figsize`, `legend=True`, `title`\n",
    "    \n",
    "    \n",
    "* Overall appearance of the plots: `plt.style.available`. Using one `plt.style.use('ggplot')`-Nice appearence\n",
    "\n",
    "\n",
    "* **Tidy data**: If we have tidy data we can groupby by 1 column and then make `bar` plots\n",
    "    * If we have multilevel grouping leading to multilevel index then plotting would be ugly. To avoid multilevel index use `df.pivot_table()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Seaborn New\n",
    "Seaborn API has 5 types of plot. Ted made it simple into 3 types.\n",
    "\n",
    "* Generic code: `sns.plotting_func(x='col1', y='col2', hue='col3', data=df)`\n",
    "    * For univariate plot use x or y. hue ( pass a categorical column) adds dimensionality by splitting and coloring data by the categorical column elements\n",
    "    <p></p> \n",
    "    <p></p>\n",
    "#### Seaborn divides plotting into following groups: seaborn.pydata.org/api.html\n",
    "\n",
    "    * `Relational` | `categorical` | `Distribution` | `Regression` | `Matrix`\n",
    "    <p></p>\n",
    "     * **Relational**: `sns.relplot()` with `kind=scatter`(default) `kind=line`\n",
    "     <p></p>\n",
    "     * **Categorical**: `sns.catplot()` <br>\n",
    "         * **Categorical scatterplot**: with `kind=strip`(default-stripplot), `kind=swarm` (swarmplot)<br>\n",
    "         * **Categorical distribution plot**:  with `kind=box`(default-boxplot), `kind=violin` (violinplot),  `kind=boxen` (boxenplot)<br>\n",
    "         * **Categorical estimate plot**:  with `kind=point`(default-pointplot), `kind=bar` (barplot),  `kind=count` (countplot)\n",
    "     <p></p>\n",
    "     * **Categorical**: `sns.jointplot()` | `sns.pairplot()` | `sns.distplot()` | `sns.kdeplot()`\n",
    "     <p></p>\n",
    "     * **Regression**: `sns.lmplot()` | `sns.regplot()` | `sns.residplot()` \n",
    "     <p></p>\n",
    "     * **Matrix**: `sns.heatmap()` | `sns.clustermap()`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ted divided plotting into-\n",
    "1. **Distribution**: For continuous variable `box`, `violin`, `hist`, `kde` <br>\n",
    "2. **Grouping & aggregating**: Group by a categorical variable and aggregate a continuous variable `bar`, `count`, `point` <br>\n",
    "3. **Raw data**: `scatter`, `line`, heatmaps \n",
    " \n",
    " \n",
    "* **Distribution plot**:\n",
    "    * Common: `boxplot`, `violinplot`, `distplot`\n",
    "    * Others: `stripplot`, `swarmplot`, `boxenplot`, `jointplot`\n",
    "    * **Univariate distribution plot**: \n",
    "        * `sns.boxplot()` | `sns.violinplot()` | `sns.distplot(df['col'], hist=False)`-kde or hist\n",
    "    * **MULTIvariate distribution plot**: \n",
    "        * `sns.boxplot(x=continuous_col, y=categorical_col, data=, ax=ax)`. Now use this `ax` and 01. matplotlib fundamentals to optimize the plot further\n",
    "        * `sns.violinplot(x=continuous_col, y=categorical_col, data=, ax=ax)`\n",
    "        * `sns.jointplot(x=continuous_col, y=continuous_col, data=, ax=ax)`\n",
    "        * Another dimension can be added by using `hue`\n",
    "      \n",
    "      \n",
    "* **Grouping & aggregating**:\n",
    "    * **Univariate plot**: `sns.countplot(y=categorical_col, data=)`. `hue` can be used too\n",
    "    * **Multivariate plot- Grouping by a categorical**:\n",
    "        * `sns.pointplot(x=categorical_col, y=continuous_col, data=, estimator=np.mean, ci=None)` estimator we can use any other function, ci is confidence interval. Another dimension could be added by `hue` <br>\n",
    "        * `sns.barplot(x=categorical_col, y=continuous_col, data=, estimator=np.mean, ci=None)` estimator we can use any other function, ci is confidence interval. Another dimension could be added by `hue`\n",
    "        \n",
    "        \n",
    "        \n",
    "* **Raw plots**: Do not change the data. Plot the raw data as it is.\n",
    "    * `scatterplot` | `lineplot` | `regplot` or `lmplot` | `heatmap` or `clustermap`\n",
    "    * `sns.scatterplot(x=continuous_col, y=continuous_col, data=, hue=categorical_col, style=categorical_col)`\n",
    "    * `sns.regplot(x=continuous_col, y=continuous_col, data=)`\n",
    "    * `sns.lmplot(x=continuous_col, y=continuous_col, hue=, data=, lowess=True)` lowess=True allows locally weighted regression\n",
    "    \n",
    "    * **Heatmaps**: Mostly used to see correlation between different variables in the data\n",
    "        * `sns.heatmap(df.corr(), ax=ax)`\n",
    "        * `sns.clustermap(df.corr(), ax=ax)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Grid plots** Multiple plots could be created by `sns.relplot()` | `sns.catplot()` | `sns.lmplot()`. These 3 do not create any new type of plots just allows to create a grid of multiple plots <br>\n",
    "    * `sns.catplot(x=continuous_col, data, kind=, col=categorical_col)` `col_wrap=3` maximum 3 columns in each row\n",
    "    *  `sns.catplot(x=categorical_col, y= continuous_col, hue=categorical_col, data=, kind='bar', row=categorical_col, col=categorical_col, col_wrap=3, ci=None)`\n",
    "    \n",
    "\n",
    "* Same grid plots with `row`, `col` can be done with `relplot()` and `lmplot()`. However here x and y should be continuous\n",
    "    * `sns.relplot(x=continuous_col, y= continuous_col, hue=categorical_col, data=, kind='scatter', row=categorical_col, col=categorical_col, col_wrap=3)`    \n",
    "    * `sns.lmplot(x=continuous_col, y= continuous_col, hue=categorical_col, data=, row=categorical_col, col=categorical_col, col_wrap=3)` No `kind` here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 Dexplot\n",
    "`import dexplot as dxp`\n",
    "* `aggplot` | `jointplot` | `heatmap`\n",
    "\n",
    "\n",
    "* **`aggplot`**: similar to `catplot`\n",
    "    * `dxp.aggplot(agg='continuous_col', groupby='categorical_col', data=, hue='categorical_col')` Other parameters are `figsize=(12,8)` and make **stacked bar plot** by adding `stacked=True`\n",
    "    * By default `aggfunc='mean'` we can change that by passing other func `aggfunc='max'` / 'median'\n",
    "    * `orient='h'` horizontal orientation\n",
    "    *  `dxp.aggplot(agg='categorical_col', data=)` will plot a **countplot** or value count plot. We can add `hue='categorical_col`. And `normalize=all` or any of the `agg` or `hue` col\n",
    "    * we can also plot other than `bar` plot: `line` | `box` | `hist`\n",
    "        * `dxp.aggplot(agg='continuous', groupby='categorical', data=, hue='categorical', kind='line', aggfunc='median')` `orient='h'` can be an add on | `kind='box'` \n",
    "        * `dxp.aggplot(agg='continuous', groupby='categorical', data=, kind='hist', orient='v')` OR `kind='kde'`\n",
    "        * Use both `rows` and `cols`:\n",
    "            * `dxp.aggplot(agg='continuous', groupby='categorical', data=, kind='hist', row='categorical', col='categorical')` OR we can use `normalize` with `kind='bar'`\n",
    "            \n",
    "            \n",
    "* **`jointplot`**: plots raw data. No aggregation of data in this\n",
    "    * `dxp.jointplot(continuous1, continuous2, data=, hue=categorical1, row=categorical2, col=categorical3)`\n",
    "    * Add `fit_reg=True` to add a regression line\n",
    "    * Default `kind=scatter`. We can also use `kind=line` eg- `dxp.jointplot(date, continuous2, data=, hue=categorical1, row=categorical2, col=categorical3, kind='line')`\n",
    "    \n",
    "\n",
    "            \n",
    "* **`heatmap`**: `dxp.heatmap(categorical1, categorical2, data=, annot=True)`\n",
    "    * **Aggregate and heatmap**: `dxp.heatmap(x=categorical1, y=categorical2, agg=continuous1, aggfunc='max', data=, annot=True)`\n",
    "        * We can also **normalize** by either x or y i.e. a categorical column `normalize=categorical1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:green; font-size:40px\">10. EDA </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Taxonomy\n",
    "* variables\n",
    "    * Continuous\n",
    "    * Categorical\n",
    "        * Ordinal\n",
    "        * Nominal\n",
    "        \n",
    "        \n",
    "        \n",
    "* Convert a faeture/variable into categorical: \n",
    "    * `cat = pd.Categorical(df['col'])` |  `cat = df['col'].astype('category')` \n",
    "    * If we want an ordered category: \n",
    "        * If order in the `'col'` would be `order=['fair', 'good', 'very good']`\n",
    "        * `cat = pd.Categorical(df['col'], categories=order, ordered=True)`\n",
    "        * **Binning a numerical feature leads to ordered category**\n",
    "            * `pd.cut(df['col'], 5)` | `pd.cut(df['col'], [0,2,4,6])`\n",
    "            * `pd.qcut(df['col'], 5)` quantile cut leads to bins such that each contains equal no. of observation\n",
    "    * Conversion of categorical feature to categorical (rather than keeping them as object datatype) saves a lot of space. Conversion to a Categorical feature will map a unique element in the category to an integer and assign that integer every time it encounters that element. Where as the object column will store the 'string' everytime.\n",
    "    * we can access following attributes- `cat.describe()` gives counts and frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Dictionary\n",
    "\n",
    "* Creating data dictionary with col_name, col_description, df_dtypes, col_nunique\n",
    "\n",
    "* Label each column as continuous, ordinal, nominal\n",
    "\n",
    "* Rearrage column order: \n",
    "    * atfirst strring column that describves the row \n",
    "    * Left to right should be more important to less important\n",
    "    * Group similar columns together\n",
    "    \n",
    "* Add missing values per column in data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span  style=\"color:RED; font-size:22px\"> Step wise progress of how to systematically analyze a data upto EDA </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Non-Graphical Univariate Analysis\n",
    "\n",
    "* Steps after one get a data\n",
    "    * Read in data and data-dictionary\n",
    "    * Tidy the data\n",
    "    * Update data dictionary with `dtype` | `nuniuqe` | `[continuous, ordinal, norminal]` | `missing values`\n",
    "    * Rearrange data column orders\n",
    "\n",
    "\n",
    "\n",
    "| Univariate             | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical | Bar char of frequencies (count/percent) | `value_counts` (count/percent) |\n",
    "| Continuous  | Histogram/KDE, box/violin  | central tendency -mean/median/mode, variance, std, skew, IQR  |\n",
    "\n",
    "| Multivariate            | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical vs Categorical | heat map, mosaic plot | Cross tabulation (count/percent) |\n",
    "| Continuous vs Continuous  | all pairwise scatterplots, kde, heatmaps |  all pairwise correlation/regression   |\n",
    "| Categorical vs Continuous  | All seaborn \"categorical\" plots | Summary statistics for each group |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Non-GRAPHICAL Univariate analysis for continuous variables:**\n",
    "    * `df.descrtibe()` will include the central tendencies\n",
    "    \n",
    "    \n",
    "* **Non-GRAPHICAL Univariate analysis for categorical variables:**\n",
    "    * Ordinal: `df['col'].value_counts()` and then convert them into categorical dtype by\n",
    "        * `order=['fair', 'good', 'very good']`\n",
    "        * `cat = pd.Categorical(df['col'], categories=order, ordered=True)`\n",
    "    * Nominal:  `df['col'].value_counts()` we can convert them in categorical dtype but without order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Graphical Univariate Analysis\n",
    "\n",
    "* **GRAPHICAL Univariate analysis for continuous variables:**\n",
    "    * `df.descrtibe()` \n",
    "    * **Distribution** : We can plot histogram or kde plot separately with matplotlib. Seaborn plots them together `sns.distplot(df['numeric_col])` we can make only 1 show by `kde=False` or `hist=False`. We can also use **violin plot**\n",
    "    * **Outliers**: Check by boxplot `sns.boxplot(df['numeric_col'], data=)` \n",
    "    * Consider binning continuous variable into a categorical column\n",
    "\n",
    "\n",
    "\n",
    "* **GRAPHICAL Univariate analysis for categorical variables:**\n",
    "    * `value_counts()` and plot\n",
    "    * `sns.countplot()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Outliers and Duplicates\n",
    "**Numerical columns**\n",
    "* Follow Soledad's way of finding Outliers and how to find out upper and lower boundary\n",
    "* Follow Ted's way of making a boolean series of which rows contain an outlier \n",
    "    * explore outlier rows to check for a trend\n",
    "* Plot entire df boxplot to visually see outliers: \n",
    "    * `df.plot(kind='box', subplots=True, figsize=(8,6), layout=(2,4))`\n",
    "* Once found explore the outlier rows and see if there is something abnormal\n",
    "* We can capture **Outlierness** by creating a 1/0 column\n",
    "\n",
    "**Categorical columns**\n",
    "* Look into low frequency elements in a category. Aggregate all low frequency elements into one `'others'` to reduce cardinality ~ Soldedad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Duplicates\n",
    "* **Finding duplicate rows**\n",
    "    * `filt=df.duplicated(keep=False)` then `df[filt]` to see duplicate rows\n",
    "    * Drop duplicate rows by `df.drop_duplicates()`\n",
    "    \n",
    "* **Finding duplicate columns**\n",
    "    * `filt=df.T.duplicated(keep=False)` then `df.T[filt]` to see duplicate columns\n",
    "    * Follow Soledad's method to remove constant and quasi-constant values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Multivariate Categorical vs Categorical\n",
    "\n",
    "* **Non-GRAPHICAL categorical vs categorical:**\n",
    "    * `df.pivot_table(index='categorical_col1', columns='categorical_col2', aggfunc='size')`\n",
    "    \n",
    "    \n",
    "* **GRAPHICAL categorical vs categorical:**\n",
    "    * `sns.countplot(x='categorical_col1', hue='categorical_col2', data=)`\n",
    "    * This is same as `df.pivot_table(index='categorical_col1', columns='categorical_col2', aggfunc='size').plot(kind='bar')`\n",
    "    * Or we can do heatmaps\n",
    "        * `cat_abc=df.pivot_table(index='categorical_col1', columns='categorical_col2', aggfunc='size')`\n",
    "        * `sns.heatmap(cat_abc)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Univariate             | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical | Bar char of frequencies (count/percent) | `value_counts` (count/percent) |\n",
    "| Continuous  | Histogram/KDE, box/violin  | central tendency -mean/median/mode, variance, std, skew, IQR  |\n",
    "\n",
    "| Multivariate            | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical vs Categorical | heat map, mosaic plot | Cross tabulation (count/percent) |\n",
    "| Continuous vs Continuous  | all pairwise scatterplots, kde, heatmaps |  all pairwise correlation/regression   |\n",
    "| Categorical vs Continuous  | All seaborn \"categorical\" plots | Summary statistics for each group |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Multivariate Categorical vs Continuous\n",
    "* Mean value of the continous variable w.r.t each categorical element\n",
    "    * `sns.barplot('categorical_Col', 'continuous_col', data=)`\n",
    "    * Add 1 more dimension (categorical) by `hue` or `row` or `col`\n",
    "        * `sns.catplot(x='categorical_l', y='continuous_col', data=, kind-='bar', row='categorical_2', col='categorical_3')`\n",
    "    * Heatmap-2 categorical and 1 continuous column\n",
    "        * `pt = df.pivot_table(index='categorical_1', columns='categorical_2', values='continuous')`\n",
    "        * `sns.heatmap(pt)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Multivariate Continuous vs Continuous\n",
    "* Pairwise scatter plot\n",
    "* clustermap to find cluster of similar features in the data\n",
    "    * `sns.clustermap(df.corr())`\n",
    "* When 2 continuous variable, go to plot is scatter plot\n",
    "    * Also, `sns.lmplot('continuous_col','continuous_col', data=)`\n",
    "    * `sns.lmplot('continuous_col','continuous_col', data=, hue='categorical_1', fit_reg=False)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Binning a continuous variable\n",
    "\n",
    "* **Binning a numerical feature leads to ordered category**\n",
    "    * `pd.cut(df['col'], 5)` | `pd.cut(df['col'], [0,2,4,6])`\n",
    "    * `pd.qcut(df['col'], 5)` quantile cut leads to bins such that each contains equal no. of observation"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
