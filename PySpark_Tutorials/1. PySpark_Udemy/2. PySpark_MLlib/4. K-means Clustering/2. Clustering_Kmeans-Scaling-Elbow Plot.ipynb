{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Consulting Project \n",
    "\n",
    "A large technology firm needs your help, they've been hacked! Luckily their forensic engineers have grabbed valuable data about the hacks, including information like session time,locations, wpm typing speed, etc. The forensic engineer relates to you what she has been able to figure out so far, she has been able to grab meta data of each session that the hackers used to connect to their servers. These are the features of the data:\n",
    "\n",
    "* 'Session_Connection_Time': How long the session lasted in minutes\n",
    "* 'Bytes Transferred': Number of MB transferred during session\n",
    "* 'Kali_Trace_Used': Indicates if the hacker was using Kali Linux\n",
    "* 'Servers_Corrupted': Number of server corrupted during the attack\n",
    "* 'Pages_Corrupted': Number of pages illegally accessed\n",
    "* 'Location': Location attack came from (Probably useless because the hackers used VPNs)\n",
    "* 'WPM_Typing_Speed': Their estimated typing speed based on session logs.\n",
    "\n",
    "\n",
    "The technology firm has 3 potential hackers that perpetrated the attack. Their certain of the first two hackers but they aren't very sure if the third hacker was involved or not. They have requested your help! Can you help figure out whether or not the third suspect had anything to do with the attacks, or was it just two hackers? It's probably not possible to know for sure, but maybe what you've just learned about Clustering can help!\n",
    "\n",
    "**One last key fact, the forensic engineer knows that the hackers trade off attacks. Meaning they should each have roughly the same amount of attacks. For example if there were 100 total attacks, then in a 2 hacker situation each should have about 50 hacks, in a three hacker situation each would have about 33 hacks. The engineer believes this is the key element to solving this, but doesn't know how to distinguish this unlabeled data into groups of hackers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('clustering').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
      "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|            Location|WPM_Typing_Speed|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
      "|                    8.0|           391.09|              1|             2.96|            7.0|            Slovenia|           72.37|\n",
      "|                   20.0|           720.99|              0|             3.04|            9.0|British Virgin Is...|           69.08|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('hack_data.csv', header=True, inferSchema=True)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT Location)|\n",
      "+------------------------+\n",
      "|                     181|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.agg(countDistinct(df['Location'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "too many locations, no point of categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Session_Connection_Time',\n",
       " 'Bytes Transferred',\n",
       " 'Kali_Trace_Used',\n",
       " 'Servers_Corrupted',\n",
       " 'Pages_Corrupted',\n",
       " 'Location',\n",
       " 'WPM_Typing_Speed']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['Session_Connection_Time',\n",
    " 'Bytes Transferred',\n",
    " 'Kali_Trace_Used',\n",
    " 'Servers_Corrupted',\n",
    " 'Pages_Corrupted',\n",
    " 'WPM_Typing_Speed'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37, features=DenseVector([8.0, 391.09, 1.0, 2.96, 7.0, 72.37]))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = assembler.transform(df)\n",
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------+----------------+--------------------+\n",
      "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|Location|WPM_Typing_Speed|            features|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------+----------------+--------------------+\n",
      "|                    8.0|           391.09|              1|             2.96|            7.0|Slovenia|           72.37|[8.0,391.09,1.0,2...|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------+----------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As algorithm depends on distance, we need to scale them as sale of features vary\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------+----------------+--------------------+--------------------+\n",
      "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|Location|WPM_Typing_Speed|            features|     scaled_features|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------+----------------+--------------------+--------------------+\n",
      "|                    8.0|           391.09|              1|             2.96|            7.0|Slovenia|           72.37|[8.0,391.09,1.0,2...|[0.56785108466505...|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------+----------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scale = StandardScaler(inputCol='features', \n",
    "                       outputCol='scaled_features', \n",
    "                       withMean=False, withStd=True)\n",
    "df_scaled = scale.fit(df2).transform(df2)\n",
    "df_scaled.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering model and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 hackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_2 error 601.7707512676716\n"
     ]
    }
   ],
   "source": [
    "# No label, no point of splitting data\n",
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans2 = KMeans(featuresCol='scaled_features',\n",
    "                 predictionCol='prediction2',k=2)\n",
    "model2 = kmeans2.fit(df_scaled)\n",
    "\n",
    "# Within Set Sum of Squared Errors.\n",
    "wsse2 = model2.computeCost(df_scaled)\n",
    "print(f'model_2 error {wsse2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 hackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_3 error 434.75507308487647\n"
     ]
    }
   ],
   "source": [
    "kmeans3 = KMeans(featuresCol='scaled_features',\n",
    "                 predictionCol='prediction3',k=3)\n",
    "model3 = kmeans3.fit(df_scaled)\n",
    "\n",
    "wsse3 = model3.computeCost(df_scaled)\n",
    "print(f'model_3 error {wsse3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.26023837, 1.31829808, 0.99280765, 1.36491885, 2.5625043 ,\n",
       "        5.26676612]),\n",
       " array([2.99991988, 2.92319035, 1.05261534, 3.20390443, 4.51321315,\n",
       "        3.28474   ])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.26023837, 1.31829808, 0.99280765, 1.36491885, 2.5625043 ,\n",
       "        5.26676612]),\n",
       " array([3.05623261, 2.95754486, 1.99757683, 3.2079628 , 4.49941976,\n",
       "        3.26738378]),\n",
       " array([2.93719177, 2.88492202, 0.        , 3.19938371, 4.52857793,\n",
       "        3.30407351])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As no. of hacking by each hacker is almost same, we need to see how many times cluster specific to each hacker shows up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 HACKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|     scaled_features|prediction2|\n",
      "+--------------------+-----------+\n",
      "|[0.56785108466505...|          0|\n",
      "|[1.41962771166263...|          0|\n",
      "|[2.20042295307707...|          0|\n",
      "|[0.14196277116626...|          0|\n",
      "|[1.41962771166263...|          0|\n",
      "|[0.07098138558313...|          0|\n",
      "|[1.27766494049636...|          0|\n",
      "|[1.56159048282889...|          0|\n",
      "|[1.06472078374697...|          0|\n",
      "|[0.85177662699757...|          0|\n",
      "|[1.06472078374697...|          0|\n",
      "|[2.27140433866020...|          0|\n",
      "|[1.63257186841202...|          0|\n",
      "|[0.63883247024818...|          0|\n",
      "|[1.91649741074455...|          0|\n",
      "|[0.85177662699757...|          0|\n",
      "|[1.49060909724576...|          0|\n",
      "|[0.70981385583131...|          0|\n",
      "|[1.41962771166263...|          0|\n",
      "|[1.56159048282889...|          0|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred = model2.transform(df_scaled)\n",
    "df_pred.select(df_pred['scaled_features'], df_pred['prediction2']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|prediction2|count|\n",
      "+-----------+-----+\n",
      "|          1|  167|\n",
      "|          0|  167|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred.groupBy('prediction2').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 HACKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|     scaled_features|prediction3|\n",
      "+--------------------+-----------+\n",
      "|[0.56785108466505...|          0|\n",
      "|[1.41962771166263...|          0|\n",
      "|[2.20042295307707...|          0|\n",
      "|[0.14196277116626...|          0|\n",
      "|[1.41962771166263...|          0|\n",
      "|[0.07098138558313...|          0|\n",
      "|[1.27766494049636...|          0|\n",
      "|[1.56159048282889...|          0|\n",
      "|[1.06472078374697...|          0|\n",
      "|[0.85177662699757...|          0|\n",
      "|[1.06472078374697...|          0|\n",
      "|[2.27140433866020...|          0|\n",
      "|[1.63257186841202...|          0|\n",
      "|[0.63883247024818...|          0|\n",
      "|[1.91649741074455...|          0|\n",
      "|[0.85177662699757...|          0|\n",
      "|[1.49060909724576...|          0|\n",
      "|[0.70981385583131...|          0|\n",
      "|[1.41962771166263...|          0|\n",
      "|[1.56159048282889...|          0|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred = model3.transform(df_scaled)\n",
    "df_pred.select(df_pred['scaled_features'], df_pred['prediction3']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|prediction3|count|\n",
      "+-----------+-----+\n",
      "|          1|   88|\n",
      "|          2|   79|\n",
      "|          0|  167|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred.groupBy(df_pred['prediction3']).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When 3 hackers I don't see equal number of hacking so it doesn't match the criteria asked\n",
    "    \n",
    "Error is also much low \n",
    "\n",
    "So, no of clusters/hackers is most likely 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For loop of checking errors with different clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of clusters 2\n",
      "wsse with 2 no. of clusters: 601.7707512676716\n",
      "\n",
      "No. of clusters 3\n",
      "wsse with 3 no. of clusters: 434.75507308487647\n",
      "\n",
      "No. of clusters 4\n",
      "wsse with 4 no. of clusters: 267.1336116887891\n",
      "\n",
      "No. of clusters 5\n",
      "wsse with 5 no. of clusters: 248.97305882286832\n",
      "\n",
      "No. of clusters 6\n",
      "wsse with 6 no. of clusters: 227.2036624315653\n",
      "\n",
      "No. of clusters 7\n",
      "wsse with 7 no. of clusters: 214.4706560026703\n",
      "\n",
      "No. of clusters 8\n",
      "wsse with 8 no. of clusters: 196.7633646545008\n",
      "\n",
      "No. of clusters 9\n",
      "wsse with 9 no. of clusters: 184.7017624465714\n",
      "\n",
      "No. of clusters 10\n",
      "wsse with 10 no. of clusters: 179.90466857065763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(2,11):\n",
    "    print(f'No. of clusters {k}')\n",
    "    kmeans = KMeans(featuresCol='scaled_features', \n",
    "                    predictionCol='prediction', k=k)\n",
    "    model = kmeans.fit(df_scaled)\n",
    "    wsse = model.computeCost(df_scaled)\n",
    "    print(f'wsse with {k} no. of clusters: {wsse}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on error value 4 clusters would be better. And that is where domain knowledge is much better than elbow method. Because if we would have followed the elbow method we will end up with 4 clusters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
